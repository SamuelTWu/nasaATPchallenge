{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Output\n",
    "This script will comb through the FUSAR runways_data_set.csv data and produce as output a table of form:\n",
    "\n",
    "| AIRPORT | DATE | TIME (15 min intervals) | # arrivals |\n",
    "| ------- | ---- | ----------------------- | ---------- |\n",
    "\n",
    "_In the final output, we will need to convert from TIME (15 min intervals) to BASETIME_BUCKET where BASETIME = TIME and BUCKET is 15 minute intervals._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from datetime import date, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekday(day):\n",
    "    match day:\n",
    "        case 0:\n",
    "            return 'monday'\n",
    "        case 1:\n",
    "            return 'tuesday'\n",
    "        case 2:\n",
    "            return 'wednesday'\n",
    "        case 3:\n",
    "            return 'thursday'\n",
    "        case 4:\n",
    "            return 'friday'\n",
    "        case 5:\n",
    "            return 'saturday'\n",
    "        case 6:\n",
    "            return 'sunday'\n",
    "        case _:\n",
    "            return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cyclical_time(time):\n",
    "    hour = int(time[0:2])\n",
    "    minute = int(time[2:])\n",
    "    abs_time = (hour*3600)+(minute*60)\n",
    "    return math.sin((abs_time/86400)*math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(x, a):\n",
    "    return math.sin((x/a)*math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket(time):\n",
    "    #given an arbitrary time t (hhmm), returns closest 15 minute interval\n",
    "    #example: 0112 -> 0130\n",
    "    hour = int(time[:2])\n",
    "    minute = int(time[2:])\n",
    "    if minute == 0:\n",
    "        return str((hour)%24).zfill(2)+\"00\"\n",
    "    elif minute <= 15 and minute > 0:\n",
    "        return str(hour%24).zfill(2)+\"15\"\n",
    "    elif minute > 15 and minute <= 30:\n",
    "        return str(hour%24).zfill(2)+\"30\"\n",
    "    elif minute > 30 and minute <= 45:\n",
    "        return str(hour%24).zfill(2)+\"45\"\n",
    "    elif minute > 45:\n",
    "        return str((hour+1)%24).zfill(2)+\"00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given time t, increases to next bucket\n",
    "#ex: 0115 -> 0130\n",
    "def increment_time(time):\n",
    "    time = get_bucket(time)\n",
    "    hour = int(time[:2])\n",
    "    minute = int(time[2:])\n",
    "    match minute:\n",
    "        case 00:\n",
    "            return str(hour%24).zfill(2)+\"15\"\n",
    "        case 15:\n",
    "            return str(hour%24).zfill(2)+\"30\"\n",
    "        case 30:\n",
    "            return str(hour%24).zfill(2)+\"45\"\n",
    "        case 45:\n",
    "            return str((hour+1)%24).zfill(2)+\"00\"\n",
    "        case _:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(file, row):\n",
    "    airport = file.split(\"_\")[0]\n",
    "    #date = (YYMMDD)\n",
    "    date = row.split(\" \")[0].split(\"-\")[0][2:]+row.split(\" \")[0].split(\"-\")[1]+row.split(\" \")[0].split(\"-\")[2]\n",
    "    time = get_bucket(row.split(\" \")[-1].split(\":\")[0]+row.split(\" \")[-1].split(\":\")[1])\n",
    "    return airport, date, time              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 2000 # dummy leap year to allow input X-02-29 (leap day)\n",
    "seasons = [('winter', (date(Y,  1,  1),  date(Y,  3, 20))),\n",
    "           ('spring', (date(Y,  3, 21),  date(Y,  6, 20))),\n",
    "           ('summer', (date(Y,  6, 21),  date(Y,  9, 22))),\n",
    "           ('autumn', (date(Y,  9, 23),  date(Y, 12, 20))),\n",
    "           ('winter', (date(Y, 12, 21),  date(Y, 12, 31)))]\n",
    "def get_season(now):\n",
    "    if isinstance(now, datetime):\n",
    "        now = now.date()\n",
    "    now = now.replace(year=Y)\n",
    "    return next(season for season, (start, end) in seasons\n",
    "                if start <= now <= end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_nans(df):\n",
    "    #given a df, for each column, replace nans with averages \n",
    "    # Use linear interpolation to fill NaNs, then limit the interpolation to \"both\"\n",
    "    # so that it only fills NaNs that are bounded by known values above and below.\n",
    "    df_interpolated = df.infer_objects(copy=False)\n",
    "    df_interpolated = df_interpolated.apply(lambda col: col.interpolate(method='linear'))\n",
    "    df_interpolated = df_interpolated.ffill().bfill()\n",
    "    return df_interpolated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a dataframe with cols [[airport_date_time][# arrivals]]\n",
    "#target data\n",
    "def parse_runways_data(folder_paths):\n",
    "    extension = 'runways_data_set.csv'\n",
    "    target_data = {}\n",
    "    # Search for files in the specified folder\n",
    "    for folder_path in folder_paths:\n",
    "        for root, dirs, files in os.walk(\"data/\"+folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith(extension):\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    #drop duplicates\n",
    "                    df = df.drop_duplicates(subset='gufi', keep='last').reset_index(drop=True)\n",
    "                    for ind, row in df.iterrows():\n",
    "                        if not pd.isna(df.loc[ind, 'arrival_runway_actual_time']):\n",
    "                            airport, date, time = convert_timestamp(file, df[\"arrival_runway_actual_time\"][ind])\n",
    "                            key = airport+\"_\"+date+\"_\"+time\n",
    "                            if key in target_data.keys():\n",
    "                                target_data[key]+=1\n",
    "                            else:\n",
    "                                target_data[key] = 0\n",
    "    return pd.DataFrame.from_dict(target_data, orient='index', columns=['# arrivals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_TFM_track_data(folder_paths):\n",
    "    #slightly more complicated than I thought. There's multiple estimates for each flight, so we need to get the last estimate for each flight. \n",
    "    extension = 'TFM_track_data_set.csv'\n",
    "    target_data = {}\n",
    "\n",
    "    # Search for files in the specified folder\n",
    "    for folder_path in folder_paths:\n",
    "        for root, dirs, files in os.walk(\"data/\"+folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith(extension):\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    #keep only the most recent estimate\n",
    "                    df = df.drop_duplicates(subset='gufi', keep='last').reset_index(drop=True)\n",
    "                    for ind, row in df.iterrows():\n",
    "                        if not pd.isna(df.loc[ind, 'arrival_runway_estimated_time']):\n",
    "                            airport, date, time = convert_timestamp(file, df[\"arrival_runway_estimated_time\"][ind])\n",
    "                            key = airport+\"_\"+date+\"_\"+time\n",
    "                            if key in target_data.keys():\n",
    "                                target_data[key]+=1\n",
    "                            else:\n",
    "                                target_data[key] = 0\n",
    "    return pd.DataFrame.from_dict(target_data, orient='index', columns=['estimated # arrivals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns dataframe with cols [airport_date_time][][] \n",
    "#weather data\n",
    "def parse_LAMP_data(folder_paths):\n",
    "    extension = 'LAMP_data_set.csv'\n",
    "    weather_data = {}\n",
    "    #weather data is forecasts from timestamp t to n steps in the future. This only uses forecast n=1 steps into the future. \n",
    "\n",
    "    # Search for files in the specified folder\n",
    "    for folder_path in folder_paths:\n",
    "        for root, dirs, files in os.walk(\"data/\"+folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith(extension):\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    for ind in range(0, len(df), 25):\n",
    "                        for i in range(4):\n",
    "                            if not pd.isna(df.loc[ind+i, 'forecast_timestamp']):\n",
    "                                airport, date, time = convert_timestamp(file, df[\"forecast_timestamp\"][ind+i])\n",
    "                                #get the forecast data for each timestamp\n",
    "                                temperature = df[\"temperature\"][ind+i]\n",
    "                                wind_direction = df[\"wind_direction\"][ind+i]\n",
    "                                wind_speed = df[\"wind_speed\"][ind+i]\n",
    "                                wind_gust = df[\"wind_gust\"][ind+i]\n",
    "                                cloud_ceiling = df[\"cloud_ceiling\"][ind+i]\n",
    "                                visibility = df[\"visibility\"][ind+i]\n",
    "                                cloud = df[\"cloud\"][ind+i]\n",
    "                                lightning_prob = df[\"lightning_prob\"][ind+i]\n",
    "                                precip = df[\"precip\"][ind+i]\n",
    "                                #set forecast for buckets 15,30,45,00\n",
    "                                times = [time, increment_time(time), increment_time(increment_time(time)), increment_time(increment_time(increment_time(time)))]\n",
    "                                keys = [airport+\"_\"+date+\"_\"+time for time in times]\n",
    "                                for key in keys:\n",
    "                                    weather_data[key] = [temperature, wind_direction, wind_speed, wind_gust, cloud_ceiling, visibility, cloud, lightning_prob, precip]\n",
    "    return pd.DataFrame.from_dict(weather_data, orient='index', columns=['temperature', 'wind_direction', 'wind_speed', 'wind_gust', 'cloud_ceiling', 'visibility', 'cloud', 'lightning_prob', 'precip'])                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_METAR_data(folder_paths):\n",
    "    df = pd.read_csv(folder_paths\".csv\")\n",
    "    for ind, row in df.iterrows():\n",
    "        if not pd.isna(df.loc[ind+i, 'forecast_timestamp']):\n",
    "            df[]\n",
    "    return pd.DataFrame.from_dict(weather_data, orient='index', columns=['temperature', 'wind_direction', 'wind_speed', 'wind_gust', 'cloud_ceiling', 'visibility', 'cloud', 'lightning_prob', 'precip'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each row in df, add time, day, week, season data\n",
    "#for now I'm doing a timeseries, but we could change this to one-hot in the future \n",
    "def extract_time_data(df):\n",
    "    airports = []\n",
    "    times= []\n",
    "    weekdays = []\n",
    "    days = []\n",
    "    months = []\n",
    "    seasons = []\n",
    "    for ind, row in df.iterrows():\n",
    "        split_str = ind.split(\"_\")\n",
    "        airport = split_str[0]\n",
    "        date = split_str[1]\n",
    "        time = get_cyclical_time(split_str[2])\n",
    "        day = sin(datetime.strptime(date, '%y%m%d').timetuple().tm_yday, 365)\n",
    "        weekday = get_weekday(datetime.strptime(date, '%y%m%d').weekday())\n",
    "        month = date[2:4]\n",
    "        season = get_season(datetime.strptime(date, '%y%m%d'))\n",
    "        #append to list\n",
    "        airports.append(airport)\n",
    "        times.append(time)\n",
    "        weekdays.append(weekday)\n",
    "        days.append(day)\n",
    "        months.append(month)\n",
    "        seasons.append(season)\n",
    "    df['Airport'] = airports\n",
    "    df['Time'] = times\n",
    "    df['Weekday'] = weekdays\n",
    "    df['Day'] = days\n",
    "    df['Month'] = months\n",
    "    df['Season'] = seasons\n",
    "    return df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(columns_to_drop):\n",
    "    #get training dataset\n",
    "    folder_paths = ['FUSER_train/KATL', \n",
    "                    'FUSER_train/KCLT', \n",
    "                    'FUSER_train/KDEN', \n",
    "                    'FUSER_train/KDFW', \n",
    "                    'FUSER_train/KJFK', \n",
    "                    'FUSER_train/KMEM', \n",
    "                    'FUSER_train/KMIA', \n",
    "                    'FUSER_train/KORD',\n",
    "                    'FUSER_train/KPHX',\n",
    "                    'FUSER_train/KSEA'\n",
    "                    ]\n",
    "    #align weather data with target data using AIRPORT_DATE_TIME and index\n",
    "    data = pd.concat([parse_LAMP_data(folder_paths), parse_runways_data(folder_paths), parse_TFM_track_data(folder_paths)], axis=1)\n",
    "    data = data.sort_index()\n",
    "    #extract time data from index\n",
    "    data = extract_time_data(data)\n",
    "    #one-hot encode\n",
    "    data = pd.get_dummies(data, dtype=float)\n",
    "    #move '# arrivals' to end of dataframe\n",
    "    cols_at_end = ['# arrivals']\n",
    "    data = data[[c for c in data if c not in cols_at_end] \n",
    "            + [c for c in cols_at_end if c in data]]\n",
    "    #move 'estimated # arrivals' to front\n",
    "    cols = ['estimated # arrivals'] + [col for col in data.columns if col != 'estimated # arrivals']\n",
    "    data = data[cols]\n",
    "    #interpolate nans\n",
    "    data = interpolate_nans(data)\n",
    "    for col in columns_to_drop:\n",
    "        if col in data:\n",
    "            data = data.drop(col, axis=1) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(columns_to_drop):\n",
    "    #get test dataset\n",
    "    folder_paths = [\n",
    "                    'FUSER_test/KATL', \n",
    "                    'FUSER_test/KCLT', \n",
    "                    'FUSER_test/KDEN', \n",
    "                    'FUSER_test/KDFW', \n",
    "                    'FUSER_test/KJFK', \n",
    "                    'FUSER_test/KMEM', \n",
    "                    'FUSER_test/KMIA', \n",
    "                    'FUSER_test/KORD',\n",
    "                    'FUSER_test/KPHX',\n",
    "                    'FUSER_test/KSEA']\n",
    "    #align weather data with target data using AIRPORT_DATE_TIME and index\n",
    "    data = pd.concat([parse_runways_data(folder_paths),parse_LAMP_data(folder_paths), parse_TFM_track_data(folder_paths)], axis=1)\n",
    "    #sort by time:\n",
    "    data = data.sort_index()\n",
    "    #extract time data from index\n",
    "    data = extract_time_data(data)\n",
    "    #one-hot encode\n",
    "    data = pd.get_dummies(data, dtype=float)\n",
    "    #move '# arrivals' to end of dataframe\n",
    "    cols_at_end = ['# arrivals']\n",
    "    data = data[[c for c in data if c not in cols_at_end] \n",
    "            + [c for c in cols_at_end if c in data]]\n",
    "    #move 'estimated # arrivals' to front\n",
    "    cols = ['estimated # arrivals'] + [col for col in data.columns if col != 'estimated # arrivals']\n",
    "    data = data[cols]\n",
    "    #interpolate nans\n",
    "    data = interpolate_nans(data)\n",
    "    data = interpolate_nans(data)\n",
    "    for col in columns_to_drop:\n",
    "        if col in data:\n",
    "            data = data.drop(col, axis=1) \n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id(ID):\n",
    "    AIRPORT = ID.split(\"_\")[0]\n",
    "    DATE = ID.split(\"_\")[1]\n",
    "    BASETIME = ID.split(\"_\")[2]\n",
    "    BUCKET = int(ID.split(\"_\")[3])\n",
    "    for i in range(int(BUCKET/15)):\n",
    "        BASETIME = increment_time(BASETIME)\n",
    "    return AIRPORT+\"_\"+DATE+\"_\"+BASETIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(submission_format_df, predict_df):\n",
    "    #given submission_format_df, grabs the indexes and finds a predict_df index that matches\n",
    "    ids = []\n",
    "    values = []\n",
    "    for ind, row in submission_format_df.iterrows():\n",
    "        ID = submission_format_df[\"ID\"][ind]\n",
    "        #convert from \"AIPORT_DATE_BASETIME_BUCKET\" -> \"AIPORT_DATE_TIME\"\n",
    "        converted_ID = convert_id(ID)\n",
    "        value = 0\n",
    "        if converted_ID in predict_df.index:\n",
    "            value = round(predict_df.loc[converted_ID][\"Prediction\"])\n",
    "        ids.append(ID)\n",
    "        values.append(value)\n",
    "    return pd.DataFrame({\"ID\": ids, \"Value\": values})\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
